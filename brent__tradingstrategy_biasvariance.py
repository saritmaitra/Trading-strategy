# -*- coding: utf-8 -*-
"""Brent _TradingStrategy_BiasVariance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mOq6APD-sVLfxweuOJmYu_Am5tRCbbC-
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pyforest
from pyforest import *
import datetime, pickle, copy
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)
import matplotlib.pyplot as plt
# %matplotlib inline  
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
!pip install quandl
import quandl
plt.style.use('ggplot')
from statistics import variance 
from random import randint
import scipy as sp
from scipy import stats
!pip install ffn
import ffn

"""## Continuous Contracts
Individual futures contracts trade for very short periods of time, and are hence unsuitable for long-horizon analysis. Continuous futures contracts solve this problem by chaining together a series of individual futures contracts, to provide a long-term price history that is suitable for trading, behavioral and strategy analysis.
"""

# Natural Gas continuous contract
print('\033[4mBrent Crude Futures, Continuous Contract\033[0m')
BC = quandl.get("CHRIS/ICE_B1", authtoken="LSQpgUzwJRoF667ZpzyL") # natural gas continuous contract 1
BC = BC.loc['2000-01-01':,]

BC.columns

BC.shape

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline
print(BC.isnull().sum())
sns.heatmap(BC.isnull(), yticklabels = False)
plt.show()

BC.drop(['Change',
        'Wave',
        'Prev. Day Open Interest',
        'EFP Volume',
        'EFS Volume',
        'Block Volume'],
        axis=1, inplace=True)
print(BC.columns)
print('\n')
BC= BC.fillna(method='ffill')
print(BC.isnull().sum())

"""The data frame will have the index set to dates and the columns are:
- Open: The price of the first trade on the given trading day.
- High: The highest price at which a stock traded for the given trading day.
- Low: The lowest price at which a stock traded for the given trading day.
- Adj Close: According to Yahoo Finance, this is the “the closing price after adjustments for all applicable splits and dividends distributions”.
- Volume: The number of shares traded for the given trading day.
"""

BC.tail()

BC.describe()

import plotly.graph_objects as go
fig = go.Figure(data=go.Ohlc(x=BC.index,
                open=BC['Open'],
                high=BC['High'],
                low=BC['Low'],
                close=BC['Settle']))

fig.update_layout(
    title='Brent Crude Futures, Continuous Contract',
    yaxis_title='Price (USD)'
)
fig.show()

# Calculate the daily percentage change which is daily return
data = BC["2017":].copy()

print('\033[1m' + 'Daily percentage change:' + '\033[1m')
daily_ret = data['Settle'].pct_change().dropna()
mean_return = daily_ret.mean()
return_stdev = daily_ret.std()
print('Average daily return : %1.2f%% ' % round((mean_return*100),2))
print('Average Volatility : %1.2f%% ' % round((return_stdev*100), 2))

daily_ret.plot(figsize=(10,6),grid=True)
plt.title('Daily returns')
plt.show()

perf = data['Settle'].copy().calc_stats()
print('\n')
perf.display()

perf.stats

from scipy.stats import normaltest
normaltest(daily_ret)

# empirical quantile of daily returns
import scipy.stats
print('\033[4mEmpirical quantile of daily returns\033[0m')
round(daily_ret.quantile(0.05),2)

print('\033[4mCritical Values\033[0m')
n = len(daily_ret)
test_statistic = ((daily_ret.mean() - 0) / (daily_ret.std()/np.sqrt(n)))
print ('t test statistic: ', round(test_statistic,2))
print('\n')

from scipy.stats import t
p_val = 2 * (1 - t.cdf(test_statistic, n - 1))
print ('P-value is: ', round(p_val,1))
print('\n')

from scipy.stats import chi2
# Here we calculate the critical value directly because our df is too high for most chisquare tables
crit_value = chi2.ppf(0.99, (n - 1))
print ('Critical value at α = 0.01 with 251 df: ', round(crit_value,2))
print('\n')

# Plot the distributions
fig = plt.figure(figsize=(10,5))
sns.set(rc={'figure.figsize': (15,5)})
ax1 = fig.add_axes([0.1,0.1,0.8,0.8])
daily_ret.plot.hist(bins = 60)
ax1.set_xlabel("Daily returns %")
ax1.set_ylabel("Percent")
ax1.set_title("Natural Gas daily returns data")
ax1.text(-0.15,30,"Extreme Low\nreturns")
ax1.text(0.10,30,"Extreme High\nreturns")
plt.show()
print('\n')
print("Skewness : ", round(daily_ret.skew(),2))
print("Kurtosis : ", round(daily_ret.kurtosis(),2))

!pip install pyfolio
import pyfolio as pf
pf.create_simple_tear_sheet(daily_ret)

print('\033[4mProbability of +/-(1%); +/-(3%); +/-%(5) change in price (Data -> 2017- till date)\033[0m')

print ("The probability of price changes between 1%% and -1%% is %1.2f%% " % 
       (100*daily_ret[(daily_ret > -0.01) & (daily_ret < 0.01)].shape[0] / daily_ret.shape[0]))
print ("The probability of price changes between 3%% and -3%% is %1.2f%% " % 
       (100*daily_ret[(daily_ret > -0.03) & (daily_ret < 0.03)].shape[0] / daily_ret.shape[0]))
print ("The probability of price changes between 5%% and -5%% is %1.2f%% " % 
       (100*daily_ret[(daily_ret > -0.05) & (daily_ret < 0.05)].shape[0] / daily_ret.shape[0]))
print ("The probability of price changes more than 5%% is %1.2f%%" % 
       (100*daily_ret[daily_ret > 0.05].shape[0] / daily_ret.shape[0]))
print ("The probability of price changes less than -5%% is %1.2f%%" % 
       (100*daily_ret[daily_ret < -0.05].shape[0] / daily_ret.shape[0]))

print('\033[4mMinimum price [2017- till date]\033[0m')
print(round(data['Settle'].min(),2), data['Settle'].idxmin());
print('\033[4mMaximum price [2017- till date]\033[0m')
print(round(data['Settle'].max(),2), data['Settle'].idxmax());
print('\n')

print('\033[4mMinimum daily % return [2017- till date]\033[0m')
print(round(daily_ret.min(),2)*100, daily_ret.idxmin()); 
print('\033[4mMaximum daily % return [2017- till date]\033[0m')
print(round(daily_ret.max()*100, 2), daily_ret.idxmax());

# Get the number of days in `aapl`
days = (data.index[-1] - data.index[0]).days

# Calculate the CAGR 
cagr = ((((data['Settle'][-1]) / data['Settle'][1])) ** (252.0/days)) - 1

# Print the CAGR
print(round(cagr*100))

!pip install EIA_python
import eia

# Real GDP, United States, Monthly
print('\033[4mReal GDP, United States, Monthly (Number)\033[0m')
def retrieve_time_series(api, series_ID):
    """
    Return the time series dataframe, based on API and unique Series ID
    """
    #Retrieve Data By Series ID 
    series_search = api.data_by_series(series=series_ID)
    ##Create a pandas dataframe from the retrieved time series
    gdp = pd.DataFrame(series_search)
    return gdp

def main():
    """
    Run main script
    """
    try:
      #Create EIA API using specific API key
      api_key = "ad819ee5a69e69390eadf300fa168fa8"
      api = eia.API(api_key)
      #Declare desired series ID
      series_ID='STEO.GDPQXUS.M'
      gdp = retrieve_time_series(api, series_ID)
      #Print the returned dataframe df
      print(type(gdp))
      return gdp;
    except Exception as e:
      print("error", e)
      return pd.DataFrame(columns=None)

gdp = main()
gdp.tail()

gdp = gdp.rename(columns={"Real Gross Domestic Product, Monthly (billion chained 2012 dollars (seasonally-adjusted annual rate))": 
                    "gdp"})
gdp.tail()

gdp = gdp.reset_index()
gdp['Date']= pd.to_datetime(gdp['index']) 
gdp.set_index('Date', inplace=True) # setting index column
gdp = gdp.loc['2000-01-01':,['gdp']] # setting date range
gdp = gdp.astype(float)
gdp = gdp.resample('B').ffill().bfill()
gdp= gdp/21
gdp.tail()

# Real Disposable Personal Income, Monthly
print('\033[4mReal Disposable Personal Income, Monthly\033[0m')
def retrieve_time_series(api, series_ID):
    """
    Return the time series dataframe, based on API and unique Series ID
    """
    #Retrieve Data By Series ID 
    series_search = api.data_by_series(series=series_ID)
    ##Create a pandas dataframe from the retrieved time series
    rdpi = pd.DataFrame(series_search)
    return rdpi

def main():
    """
    Run main script
    """
    try:
      #Create EIA API using specific API key
      api_key = "ad819ee5a69e69390eadf300fa168fa8"
      api = eia.API(api_key)
      #Declare desired series ID
      series_ID='STEO.YD87OUS.M'
      rdpi = retrieve_time_series(api, series_ID)
      #Print the returned dataframe df
      print(type(rdpi))
      return rdpi;
    except Exception as e:
      print("error", e)
      return pd.DataFrame(columns=None)
rdpi = main()
rdpi.tail()
rdpi = rdpi.rename(columns={"Real Disposable Personal Income, Monthly (billion chained 2012 dollars (seasonally-adjusted annual rate))": 
                    "rdpi"})
rdpi = rdpi.reset_index()
rdpi['Date']= pd.to_datetime(rdpi['index']) 
rdpi.set_index('Date', inplace=True) # setting index column
rdpi= rdpi.loc['2000-01-01':,['rdpi']] # setting date range
rdpi = rdpi.astype(float)
rdpi = rdpi.resample('B').ffill().bfill()
rdpi= rdpi/21
rdpi.tail()

# Civilian Unemployment Rate, Monthly
print('\033[4mCivilian Unemployment Rate, Monthly\033[0m')
def retrieve_time_series(api, series_ID):
    """
    Return the time series dataframe, based on API and unique Series ID
    """
    #Retrieve Data By Series ID 
    series_search = api.data_by_series(series=series_ID)
    ##Create a pandas dataframe from the retrieved time series
    cur = pd.DataFrame(series_search)
    return cur

def main():
    """
    Run main script
    """
    try:
      #Create EIA API using specific API key
      api_key = "ad819ee5a69e69390eadf300fa168fa8"
      api = eia.API(api_key)
      #Declare desired series ID
      series_ID='STEO.XRUNR.M'
      cur = retrieve_time_series(api, series_ID)
      #Print the returned dataframe df
      print(type(cur))
      return cur;
    except Exception as e:
      print("error", e)
      return pd.DataFrame(columns=None)
cur = main()
cur.tail()
cur = cur.rename(columns={"Civilian Unemployment Rate, Monthly (Percent)": 
                    "cur"})
cur = cur.reset_index()
cur['Date']= pd.to_datetime(cur['index']) 
cur.set_index('Date', inplace=True) # setting index column
cur = cur.loc['2000-01-01':,['cur']] # setting date range
cur = cur.astype(float)
cur = cur.resample('B').ffill().bfill()
cur = cur/21
cur.tail()

# merging data frames
merge1 = BC.join(gdp, how='left').ffill().bfill()
merge2 = merge1.join(rdpi, how = 'left').ffill().bfill()
merge3 = merge2.join(cur, how = 'left').ffill().bfill()
merge3.info()

a = merge3[['Settle', 'gdp', 'rdpi', 'cur']]
corrMatrix = a.corr()
print(corrMatrix)
print('\n')
sns.heatmap(corrMatrix, annot=True)
plt.show()

merge3.info()

# feature engineering
merge3['day_of_week'] = merge3.index.dayofweek
merge3['day_of_month'] = merge3.index.day
merge3['quarter'] = merge3.index.quarter
merge3['month'] = merge3.index.month
merge3['year'] = merge3.index.year
merge3.tail()

# add the outcome variable, 1 if the trading session was positive (settle>open), 0 otherwise
#oil['outcome'] = np.where(oil['Open'] > oil['Settle'].shift(1), 1, 0)
# we also add three new columns ‘ho’ ‘lo’ and ‘gain’
merge3['h_o'] = merge3['High'] - merge3['Open'] # distance between Highest and Opening price
merge3['l_o'] = merge3['Low'] - merge3['Open'] # distance between Lowest and Opening price
merge3['gain'] = merge3['Settle'] - merge3['Open']

import seaborn as sns
sns.countplot(x = 'outcome', data=oil, hue='outcome')
plt.show()

merge3['100ma'] = merge3['Settle'].rolling(window=100, min_periods=0).mean()
merge3['Daily_Return'] = merge3['Settle'].pct_change()
merge3['10ma'] = merge3['Settle'].rolling(window = 10).mean()
merge3['50ma'] = merge3['Settle'].rolling(window = 50).mean()
merge3

fig = plt.figure(figsize=(10,6))
ax1 = plt.subplot2grid((6,1), (0,0), rowspan =5, colspan =1)
ax2 = plt.subplot2grid((6,1), (5,0), rowspan =1, colspan =1, sharex = ax1)
ax1.plot(oil.index, oil['Settle'])
ax1.plot(oil.index, oil['100ma'])
ax2.bar(oil.index, oil['Volume'])
plt.tight_layout()
plt.show()

"""- RSI = 100 - 100 / (1 + RS)
- where RS = Average gain over the last 14 days/Average loss over the last 14 days
"""

rsi_period = 14 
chg = merge3['Settle'].diff(1)
gain = chg.mask(chg<0,0)
loss = chg.mask(chg>0,0)
avg_gain = gain.ewm(com = rsi_period - 1, min_periods = rsi_period).mean()
avg_loss = loss.ewm(com = rsi_period - 1, min_periods = rsi_period).mean()
rs = abs(avg_gain/avg_loss)
merge3['rsi'] = 100-(100/(1+ rs))

merge3['Std_dev']=  merge3['Settle'].rolling(5).std()
merge3['ROC'] = ((merge3['Settle'] - merge3['Settle'].shift(5)) / (merge3['Settle'].shift(5)))*100
merge3['Williams%R'] = (merge3['High'].max() - merge3['Settle'])/(merge3['High'] - merge3['Low'].min()) * -100

"""### Ease of Movement (EVM)
Ease of Movement (EVM) is a volume-based oscillator which was developed by Richard Arms. EVM indicates the ease with which the prices rise or fall taking into account the volume of the security. For example, a price rise on a low volume means prices advanced with relative ease, and there was little selling pressure. Positive EVM values imply that the market is moving higher with ease, while negative values indicate an easy decline.
"""

# Ease of Movement 
dm = ((merge3['High'] + merge3['Low'])/2) - ((merge3['High'].shift(1) + merge3['Low'].shift(1))/2)
br = (merge3['Volume'] / 100000000) / ((merge3['High'] - merge3['Low']))
merge3['EVM'] = dm / br

merge3.tail()

merge3['Settle'].shift(-1)

oil = merge3.copy()
oil['outcome'] = oil['Settle'].shift(-1) - oil['Open']
oil.dropna(inplace=True)

def getBinary(value):
    if value> 0:
        return 1
    else:
        return 0

oil['outcome'] = oil['outcome'].apply(getBinary)
oil['outcome']

oil['outcome'] = oil['outcome'].apply(getBinary)
oil['outcome']

oil[['Open', 'Settle', 'outcome']].tail(10)

import seaborn as sns
sns.countplot(x = 'outcome', data=merge3, hue='outcome')
plt.show()

oil.columns

"""This is my function (based on this) to clean the dataset of nan, Inf, and missing cells (for skewed datasets)"""

def clean_dataset(oil):
    assert isinstance(oil), "df needs to be a pd.DataFrame"
    oil.fillna(-99999, inplace=True)
    indices_to_keep = ~oil.isin([np.nan, np.inf, -np.inf]).any(1)
    return oil[indices_to_keep].astype(np.float64)
oil.info()

BC= BC.astype(float)
BC.dtypes

"""### Interquartile Range :
The interquartile range (IQR), also called as midspread or middle 50%, or technically H-spread is the difference between the third quartile (Q3) and the first quartile (Q1). It covers the center of the distribution and contains 50% of the observations. 
- IQR = Q3 – Q1
- The IQR can also be used to identify the outliers in the given data set.
- The IQR gives the central tendency of the data.
"""

Q1 = BC.quantile(0.25)
Q3 = BC.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

BC.shape

BC_out = BC[~((BC < (Q1 - 1.5 * IQR)) |(BC > (Q3 + 1.5 * IQR))).any(axis=1)]
BC_out.shape

X = merge3.drop(['outcome',
             'Open',
             'High',
             'Low',
             'Settle',
             'Volume'], axis=1) # Features

from statsmodels.stats.outliers_influence import variance_inflation_factor
from joblib import Parallel, delayed
import time

k = X.copy()
k = k.replace([np.inf, -np.inf], np.nan).dropna(axis=1)

def calculate_vif_(k, thresh=10.0):
    variables = [k.columns[i] for i in range(k.shape[1])]
    dropped=True
    while dropped:
        dropped=False
        print(len(variables))
        vif = Parallel(n_jobs=-1,verbose=5)(
            delayed(variance_inflation_factor)(
                k[variables].values, ix) 
            for ix in range(len(variables)))

        maxloc = vif.index(max(vif))
        if max(vif) > thresh:
            print(time.ctime() + ' dropping \'' + k[variables].columns[maxloc] + '\' at index: ' + str(maxloc))
            variables.pop(maxloc)
            dropped=True

    print('Remaining variables:')
    print([variables])
    return k[[i for i in variables]]

z = calculate_vif_(k,10)

oil.columns

X = oil[['day_of_week', 'day_of_month', 'month', 'h_o', 'l_o', 'gain', 
         'Daily_Return', '10ma', 'Std_dev', 'ROC', 'Williams%R',]]
print(X.columns)
y = oil['outcome'].copy()

X.reset_index(inplace=True)
y = pd.DataFrame(y)
y.rename(columns = {0: 'outcome'}, inplace=True)
y.reset_index(inplace=True)

Xtrain = X.loc[X.Date <= '2018']
ytrain = y.loc[y.Date <= '2018']
Xtest = X.loc[X.Date > '2018']
ytest = y.loc[y.Date > '2018']

Xtrain.set_index('Date', inplace=True)
ytrain.set_index('Date', inplace=True)
Xtest.set_index('Date', inplace=True)
ytest.set_index('Date', inplace=True)

ytest

# Scale data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
Xtrain_scale = scaler.fit_transform(Xtrain)
print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)

np.isnan(X_train_scale).any()

np.argwhere(np.isnan(X_train_scale))

X_train_scale = np.nan_to_num(X_train_scale)
np.isnan(X_train_scale).any()

from xgboost import XGBClassifier
from sklearn import model_selection
from sklearn.linear_model import LogisticRegressionCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier

xgb = XGBClassifier()
logreg2= LogisticRegressionCV(solver='lbfgs', cv=10)
knn = KNeighborsClassifier(5)
svcl = SVC()
adb = AdaBoostClassifier()
dtclf = DecisionTreeClassifier(max_depth=5)
rfclf = RandomForestClassifier()

# prepare configuration for cross validation test harness
seed = 42
# prepare models
models = []
models.append(('LR', LogisticRegressionCV(solver='lbfgs', max_iter=5000, cv=10)))
models.append(('XGB', XGBClassifier()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('DT', DecisionTreeClassifier()))
models.append(('ADB', AdaBoostClassifier()))
models.append(('RF', RandomForestClassifier()))
models.append(('SVC', SVC()))

# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)
    cv_results = model_selection.cross_val_score(model, Xtrain_scale, np.ravel(ytrain), cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    accuracy = "%s: %.2f%% (%.2f)" % (name, cv_results.mean(), cv_results.std())
    print('Accuracy:')
    print(accuracy)

eval_set = [(Xtrain, np.ravel(ytrain)), (Xtest, np.ravel(ytest))]

# Hyperparameter optimization
xgb = XGBClassifier()

clf = xgb.fit(Xtrain, np.ravel(ytrain), 
              early_stopping_rounds=10,  
              eval_metric="logloss", 
              eval_set=eval_set)

# scores
from  sklearn.metrics import log_loss
log_train = log_loss(np.ravel(ytrain), clf.predict_proba(Xtrain)[:,1])
log_valid = log_loss(np.ravel(ytest), clf.predict_proba(Xtest)[:,1])


print('\n-----------------------')
print('  logloss train: %.5f'%log_train)
print('  logloss valid: %.5f'%log_valid)
print('-----------------------')

print('\nModel parameters...')
print(xgb.get_params())

"""ref: https://xgboost.readthedocs.io/en/latest/parameter.html

- eta [default=0.3, alias: learning_rate]

Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.

- range: [0,1]

- gamma [default=0, alias: min_split_loss]

Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.

- range: [0,∞]

- max_depth [default=6]

Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.

- range: [0,∞] (0 is only accepted in lossguided growing policy when tree_method is set as hist)

- min_child_weight [default=1]

Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.

- range: [0,∞]

- max_delta_step [default=0]

Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced. Set it to value of 1-10 might help control the update.

- range: [0,∞]

- subsample [default=1]

Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.

- range: (0,1]

- sampling_method [default= uniform]

The method to use to sample the training instances.

- uniform: each training instance has an equal probability of being selected. Typically set subsample >= 0.5 for good results.

- gradient_based: the selection probability for each training instance is proportional to the regularized absolute value of gradients (more specifically, g2+λh2−−−−−−−√). subsample may be set to as low as 0.1 without loss of model accuracy. Note that this sampling method is only supported when tree_method is set to gpu_hist; other tree methods only support uniform sampling.

- colsample_bytree, colsample_bylevel, colsample_bynode [default=1]

This is a family of parameters for subsampling of columns.

      - All colsample_by* parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.

      - colsample_bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.

      - colsample_bylevel is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.

      - colsample_bynode is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.

      - colsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.

- lambda [default=1, alias: reg_lambda]

L2 regularization term on weights. Increasing this value will make model more conservative.

- alpha [default=0, alias: reg_alpha]

L1 regularization term on weights. Increasing this value will make model more conservative.

- tree_method string [default= auto]

The tree construction algorithm used in XGBoost. 

- scale_pos_weight [default=1]

Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances). See Parameters Tuning for more discussion. Also, see Higgs Kaggle competition demo for examples: R, py1, py2, py3.

- updater [default= grow_colmaker,prune]

A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters. However, it could be also set explicitly by a user. The following updaters exist:

    - grow_colmaker: non-distributed column-based construction of trees.

    - grow_histmaker: distributed tree construction with row-based data splitting based on global proposal of histogram counting.

    - grow_local_histmaker: based on local histogram counting.
    - grow_skmaker: uses the approximate sketching algorithm
    - grow_quantile_histmaker: Grow tree using quantized histogram
    - grow_gpu_hist: Grow tree with GPU.
    - sync: synchronizes trees in all distributed nodes.
    - refresh: refreshes tree’s statistics and/or leaf values based on the current data. Note that no random subsampling of data rows is performed.
    - prune: prunes the splits where loss < min_split_loss (or gamma).

In a distributed setting, the implicit updater sequence value would be adjusted to grow_histmaker,prune by default, and you can set tree_method as hist to use grow_histmaker.

- refresh_leaf [default=1]

This is a parameter of the refresh updater. When this flag is 1, tree leafs as well as tree nodes’ stats are updated. When it is 0, only node stats are updated.

- process_type [default= default]

A type of boosting process to run.

    - Choices: default, update
    - default: The normal boosting process which creates new trees.
    - update: Starts from an existing model and only updates its trees. In each boosting iteration, a tree from the initial model is taken, a specified sequence of updaters is run for that tree, and a modified tree is added to the new model. The new model would have either the same or smaller number of trees, depending on the number of boosting iteratons performed. Currently, the following built-in updaters could be meaningfully used with this process type: refresh, prune. With process_type=update, one cannot use updaters that create new trees.

- grow_policy [default= depthwise]

Controls a way new nodes are added to the tree.

Currently supported only if tree_method is set to hist.

    - Choices: depthwise, lossguide
    - depthwise: split at nodes closest to the root.
    - lossguide: split at nodes with highest loss change.
    - max_leaves [default=0]
    - Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.
    - max_bin, [default=256]

Only used if tree_method is set to hist.

Maximum number of discrete bins to bucket continuous features.

Increasing this number improves the optimality of splits at the cost of higher computation time.

- predictor, [default=``auto``]

The type of predictor algorithm to use. Provides the same results but allows the use of GPU or CPU.

     - auto: Configure predictor based on heuristics.
     - cpu_predictor: Multicore CPU prediction algorithm.
     - gpu_predictor: Prediction using GPU. Used when tree_method is gpu_hist. When predictor is set to default value auto, the gpu_hist tree method is able to provide GPU based prediction without copying training data to GPU memory. If gpu_predictor is explicitly specified, then all data is copied into GPU, only recommended for performing prediction tasks.

- num_parallel_tree, [default=1] - Number of parallel trees constructed during each iteration. This option is used to support boosted random forest.

monotone_constraints

Constraint of variable monotonicity. See tutorial for more information.

interaction_constraints

Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information


"""

from sklearn.metrics import accuracy_score

# fit model no training data
model = XGBClassifier()
eval_set = [(Xtrain, np.ravel(ytrain)), (Xtest, np.ravel(ytest))]
model.fit(Xtrain, np.ravel(ytrain), eval_metric=["error", "logloss"], 
          early_stopping_rounds=10,eval_set=eval_set, verbose=True)
# make predictions for test data
y_pred = model.predict(Xtest)
predictions = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(np.ravel(ytest), predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# retrieve performance metrics
results = model.evals_result()
epochs = len(results['validation_0']['error'])
x_axis = range(0, epochs)
# plot log loss
fig, ax = plt.subplots()
ax.plot(x_axis, results['validation_0']['logloss'], label='Train')
ax.plot(x_axis, results['validation_1']['logloss'], label='Test')
ax.legend()
plt.ylabel('Log Loss')
plt.title('XGBoost Log Loss')
plt.show()

"""From reviewing the logloss plot, it looks like there is an opportunity to stop the learning early, perhaps somewhere around epoch 20 to epoch 40."""

# plot classification error
fig, ax = plt.subplots()
ax.plot(x_axis, results['validation_0']['error'], label='Train')
ax.plot(x_axis, results['validation_1']['error'], label='Test')
ax.legend()
plt.ylabel('Classification Error')
plt.title('XGBoost Classification Error')
plt.show()

from pprint import pprint
# Check parameters used 
print('Parameters currently in use:\n')
pprint(model.get_params())

from sklearn.model_selection import GridSearchCV

parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['binary:logistic'],
              'learning_rate': [0.1, 0.03, 0.05, 0.07], #so called `eta` value
              'max_depth': [3, 5, 6],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [100, 200, 300, 500]}

clf_grid = GridSearchCV(model,
                   parameters,
                   cv = 10,
                   n_jobs = 5,
                   verbose=True)

clf_grid.fit(Xtrain,np.ravel(ytrain))

print(clf_grid.best_score_)
print(clf_grid.best_params_)

from sklearn.metrics import accuracy_score
from sklearn.metrics import average_precision_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score

weights = (y == 0).sum() / (1.0 * (y == 1).sum())
clf = XGBClassifier(max_depth = 3,
              learning_rate= 0.03,
              min_child_weight= 4, 
              n_estimators= 100,
              nthread= 4, 
              subsample=0.7,
              colsample_bytree=0.7, 
              objective= 'binary:logistic')
kfold = model_selection.KFold(n_splits=10, shuffle=False)
results = cross_val_score(clf, Xtrain, np.ravel(ytrain), cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

probabilities = clf.fit(Xtrain, np.ravel(ytrain),
                        eval_metric = 'auc').predict_proba(Xtest)
print('AUPRC = {}'.format(round(average_precision_score(np.ravel(ytest), \
                                              probabilities[:, 1])*100, 2).astype(str) + '%'))

"""Use area under ROC as a base metric for deciding best tuned model 

This metric is a measure of how well sorted your classes are - the higher the value, the easier and more effective it is to tune the confidence level later.
"""

pip install scikit-plot

import scikitplot as skplt #to make things easy
y_pred = clf.predict_proba(Xtest)
plt.rcParams['figure.figsize'] = [10, 6]
skplt.metrics.plot_roc(ytest, y_pred)
plt.annotate('Minimum ROC Score of 50% \n (This is the minimum score to get)', 
             xy=(0.5, 0.5), 
             xytext=(0.6, 0.3),
             arrowprops=dict(shrink=0.05),
                )
plt.show()

from sklearn.metrics import explained_variance_score
predictions = clf.predict(Xtest)
predictions
#print((round(explained_variance_score(predictions,y_test),2)*100).astype(str) + '%')

predictions.shape

"""Our trading strategy waits for a positively predicted outcome to buy S&P 500 at the Opening price, and sell it at the Closing price, so our hope is to have the lowest False positives rate to avoid losses. In other words, we expect our model would have the highest precision rate."""

from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas.tseries.offsets import CustomBusinessDay
us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar() )

ytest

predictions = pd.DataFrame(predictions[-5:])
predictions.rename(columns = {0: 'predicted'}, inplace=True)
d = ytest.tail()
d.reset_index(inplace=True)
d = d.append(pd.DataFrame({'Date': pd.date_range(start=d.Date.iloc[-1], 
                                             periods=2, freq='B', closed='right')}))
d.set_index('Date', inplace=True)
predictions.index = d.tail().index
predictions.tail(1)



import graphviz
import xgboost as xgb
from xgboost import plot_tree
from matplotlib.pylab import rcParams
import os 
os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
rcParams['figure.figsize'] = 80,50
xgb.plot_tree(clf)
plt.show()

pip install shap

import shap
shap_values = shap.TreeExplainer(clf).shap_values(Xtest)
shap.summary_plot(shap_values, Xtest, plot_type="bar")

!pip install eli5
from eli5.sklearn import PermutationImportance

import eli5
# let's check the importance of each attributes
perm = PermutationImportance(clf, random_state = 0).fit(Xtest, ytest)
eli5.show_weights(perm, feature_names = Xtest.columns.tolist())

perm_train = PermutationImportance(clf, scoring='accuracy',
                                   n_iter=100, random_state=1)
# fit and see the permuation importances
perm_train.fit(Xtrain, ytrain)
eli5.explain_weights_df(perm_train, feature_names=Xtrain.columns.tolist()).head()

# figure size in inches
from matplotlib import rcParams
rcParams['figure.figsize'] = 20,5

perm_train_df = pd.DataFrame(data=perm.results_,
                                      columns=Xtrain.columns)
(sns.boxplot(data=perm_train_df)
        .set(title='Permutation Importance Distributions (training data)',
             ylabel='Importance'));
plt.xticks(rotation=90)
plt.show()

# Initialize your Jupyter notebook with initjs(), otherwise you will get an error message.
shap.initjs()
# j will be the record we explain
j = 1

explainerXGB = shap.TreeExplainer(clf)
shap_values_XGB_test = explainerXGB.shap_values(Xtest)
shap.force_plot(explainerXGB.expected_value, shap_values_XGB_test[j], Xtest.iloc[[j]])

shap.summary_plot(shap_values, Xtest)

from xgboost import plot_importance
plt.rcParams['figure.figsize'] = [12, 8]
# plot feature importance
plot_importance(clf)
plt.show()

"""Measuring feature importance by:
- Weight. The number of times a feature is used to split the data across all trees.
- Cover. The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits.
- Gain. The average training loss reduction gained when using a feature for splitting.
"""

clf.get_booster().get_score(importance_type='weight')

clf.get_booster().get_score(importance_type='cover')

clf.get_booster().get_score(importance_type='gain')

import sklearn.metrics as metrics

from sklearn.metrics import cohen_kappa_score, recall_score, roc_auc_score

# make predictions for train data and evaluate
ypred = clf.predict(Xtrain)
auc = round(roc_auc_score(ytrain, clf.predict(Xtrain))*100,2).astype(str) + '%'
print("Performance (train data) : ", auc)

# make predictions for test data and evaluate
auc = round(roc_auc_score(ytest, clf.predict(Xtest))*100,2).astype(str) + '%'
print("Performance (test data) : ", auc)
print('*'*60)

print ("\nModel Report (test data):-")
print( 'Cohen Kappa: '+ str(np.round(cohen_kappa_score(ytest, clf.predict(Xtest))*100,2).astype(str) + '%'))
print('Recall: ', round(recall_score(ytest, clf.predict(Xtest))*100, 2).astype(str) + '%')
print('*'*60)
print('\n Classification Report (test data):-\n', classification_report(ytest, clf.predict(Xtest)))

import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report 

results = confusion_matrix(ytest, clf.predict(Xtest)) 
names = ['TN','FP','FN','TP']
counts = ['{0:0.0f}'.format(value) for value in
                results.flatten()]
percentages = ['{0:.2%}'.format(value) for value in
                     results.flatten()/np.sum(results)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(names, counts, percentages)]
labels = np.asarray(labels).reshape(2,2)


plt.figure(figsize=(6,4))
sns.heatmap(results/np.sum(results), 
            annot=labels, 
            fmt='')
plt.show()

"""- False positives are cases where the model predicts a positive outcome whereas the real outcome from the testing set is negative. 
 - Vice versa, False negatives are cases where the model predicts a negative outcome where the real outcome from the test set is positive.
 - Our trading strategy waits for a positively predicted outcome to buy crude oil at the Opening price, and sell it at the Closing price, so our hope is to have the lowest False positives rate to avoid losses. 
 - In other words, we expect our model would have the highest precision rate.

Learning curves constitute a great tool to do a quick check on our models at every point in our machine learning workflow. 

When we build a model to map the relationship between the features  and the target , we assume that there is such a relationship in the first place.

The gap between the learning curves suggests some amount of variance in data. The low training AUPRC score corroborate this diagnosis of variance. The large gap and the low training error also indicates slightly overfitting problem. One more important observation we can make here is that adding new training instances is very likely to lead to better models. The validation curve doesn’t plateau at the maximum training set size used. It still has potential to decrease and converge toward the training curve. So far, we can conclude that our learning algorithm (XGB) suffers from slight variance and quite a low bias, there slightly overfitting the training data. Adding more training instances is very likely to lead to better models under the current learning algorithm.

At this point, here are a couple of things we could do to improve our model:
Adding more training instances.
Increase the regularization for our current learning algorithm. This should decrease the variance and increase the bias.
"""

from yellowbrick.model_selection import LearningCurve
visualizer = LearningCurve(clf, scoring= 'f1_weighted', cv=3)
visualizer.fit(Xtrain, np.ravel(ytrain))
visualizer.poof

from sklearn.model_selection import learning_curve

train_size, train_score, cv_score = learning_curve(\
XGBClassifier(max_depth = 3,
              learning_rate= 0.03,
              min_child_weight= 4, 
              n_estimators= 100,
              nthread= 4, 
              subsample=0.7,
              colsample_bytree=0.7, 
              objective= 'binary:logistic'), 
              Xtrain,\
              np.ravel(ytrain), 
              scoring = 'average_precision')

mean_train_score = np.mean(train_score, axis=1)
std_train_score = np.std(train_score, axis=1)
mean_cv = np.mean(cv_score, axis=1)
std_cv = np.std(cv_score, axis=1)

colours = plt.cm.tab10(np.linspace(0, 1, 9))

fig = plt.figure(figsize = (10, 6))
plt.style.use('ggplot')
plt.fill_between(train_size, 
                 (mean_train_score - std_train_score),
                 (mean_train_score + std_train_score), 
                 alpha=0.1, 
                 color=colours[0])
plt.fill_between(train_size, 
                 (mean_cv - std_cv),
                 (mean_cv + std_cv), 
                 alpha=0.1, 
                 color=colours[1])
plt.plot(train_size, 
         train_score.mean(axis = 1), 
         'o-', 
         label = 'train', \
         color = colours[0])
plt.plot(train_size, 
         cv_score.mean(axis = 1), 
         'o-', 
         label = 'cross-val', \
         color = colours[1])

ax = plt.gca()
for axis in ['top','bottom','left','right']:
    ax.spines[axis].set_linewidth(2)

handles, labels = ax.get_legend_handles_labels()
plt.legend(handles, ['train', 'cross-val'], 
           bbox_to_anchor=(0.8, 0.15), \
           loc=2, 
           borderaxespad=0, 
           fontsize = 12)
plt.xlabel('Training set size', size = 12) 
plt.ylabel('AUPRC', size = 12)
plt.title('Learning curves for xgb model', fontsize = 18, y = 1.03)
plt.show()

"""A narrow gap indicates low variance. Generally, the more narrow the gap, the lower the variance. The opposite is also true: the wider the gap, the greater the variance. Let’s now explain why this is the case. As we’ve discussed earlier, if the variance is high, then the model fits training data too well. When training data is fitted too well, the model will have trouble generalizing on data that hasn’t seen in training. When such a model is tested on its training set, and then on a validation set, the training error will be low and the validation error will generally be high. As we change training set sizes, this pattern continues, and the differences between training and validation errors will determine that gap between the two learning curves.

The relationship between the training and validation error, and the gap can be summarized this way:  So the bigger the difference between the two errors, the bigger the gap. The bigger the gap, the bigger the variance. In our case, the gap is moderately narrow and getting narrower, so we can conclude that the variance is low. High training errors are also a quick way to detect low variance. If the variance of a learning algorithm is low, then the algorithm will come up with simplistic and similar models as we change the training sets. Because the models are overly simplified, they cannot even fit the training data well (they underfit the data).

- Adding more instances (rows) to the training data is hugely unlikely to lead to better models under the current learning algorithm.
- Increasing the regularization of the current learning algorithm, if that’s the case. In a nutshell, regularization prevents the algorithm from fitting the training data too well. If we increase regularization, the model will fit training data better, and, as a consequence, the variance will decrease and the bias will increase which will eventually reduce the gap to narrow it down.
- Adding more features.
- Feature selection.
- Hyperparameter optimization.
"""

pred_prob = clf.predict_proba(Xtest[:5])
print('\033[4mProbability of prediction results\033[0m')
pred_prob = pd.DataFrame(pred_prob)
pred_prob.index = predictions.index
round(pred_prob.tail(),2)

"""Here, Sharpe is the 10 days Sharpe ratio, an important indicator of the goodness of the trading model. Considering trades expressed day by day. Here mean is the mean of the list of profit and loss, and std is the standard deviation. I have considered a risk-free return equal to 0.

the gain column contains (Close - Open) values I have written a simple helper function to plot the result of all the trades applied to the testing set and represent the total return expressed by the index basis points (not expressed in dollars $)
"""

combine['gain'] = BC['Settle'] - BC['Open']
profit_loss = combine[combine['predicted'] == 1]['gain'] 
plt.plot(profit_loss)
plt.show
print('Sharpe ratio:', round(np.sqrt(10)*(profit_loss.mean()/profit_loss.std()),2))

# Plot 
fig = plt.figure(figsize=(20,6))
plt.plot(NG['Settle'], color='gray', label='Adj  Close')
plt.plot(NG['sma1'].dropna(), label = 'sma1')
plt.plot(NG['sma2'].dropna(), label = 'sma2')
plt.plot(NG['sma3'].dropna(), label = 'sma20')
plt.plot(NG['sma4'].dropna(), label = 'sma30')
plt.plot(NG['sma5'].dropna(), label = 'sma100')
plt.ylabel('Price')
plt.xlabel('Date')
plt.title('Simple Moving Averages plot')
plt.legend(loc=0)

# Display everything
plt.show()

NG = NG[NG.index > '2018'].copy()
fig = plt.figure(figsize=(20,6))
plt.plot(NG['Settle'], color='gray', label='Adj  Close')
plt.plot(NG['sma1'].dropna(), label = 'sma1')
plt.plot(NG['sma2'].dropna(), label = 'sma2')
plt.plot(NG['sma3'].dropna(), label = 'sma20')
plt.plot(NG['sma4'].dropna(), label = 'sma30')
plt.plot(NG['sma5'].dropna(), label = 'sma100')
plt.ylabel('Price')
plt.xlabel('Date')
plt.title('Simple Moving Averages plot')
plt.legend(loc=0)

# Display everything
plt.show()

# Initialize the short and long windows
short_window = int(2)
long_window = int(30)

# Initialize the `signals` DataFrame with the `signal` column
signals = pd.DataFrame(index=NG.index)
signals['signal'] = 0.0

# Create short simple moving average over the short window
signals['short_ma'] = NG['Settle'].rolling(window=short_window, 
                                         min_periods=1, center=False).mean()

# Create long simple moving average over the long window
signals['long_ma'] = NG['Settle'].rolling(window=long_window, 
                                         min_periods=1, center=False).mean()
# Create signals
signals['signal'][short_window:] = np.where(signals['short_ma'][short_window:] 
                                            > signals['long_ma'][short_window:], 1.0, 0.0)   

# Generate trading orders
signals['positions'] = signals['signal'].diff()

# Print `signals`
print(signals)

# Initialize the plot figure
fig = plt.figure(figsize=(20,6))

# Add a subplot and label for y-axis
ax1 = fig.add_subplot(111,  ylabel='Price in $')

# Plot the closing price
NG['Settle'].plot(ax=ax1, lw=2.)

# Plot the short and long moving averages
signals[['short_ma', 'long_ma']].plot(ax=ax1, lw=2.)

# Plot the buy signals
ax1.plot(signals.loc[signals.positions == 1.0].index, 
         signals.short_ma[signals.positions == 1.0],
         '^', markersize=10, color='g')
         
# Plot the sell signals
ax1.plot(signals.loc[signals.positions == -1.0].index, 
         signals.short_ma[signals.positions == -1.0],
         'v', markersize=10, color='k')
         
# Show the plot
plt.show()

# Set the initial capital
initial_capital= float(100000.0)

# Create a DataFrame `positions`
positions = pd.DataFrame(index=signals.index).fillna(0.0)

# Buy a 100 shares
positions['NG'] = 100*signals['signal']   
  
# Initialize the portfolio with value owned   
portfolio = positions.multiply(trade_data['Settle'], axis=0)

# Store the difference in shares owned 
pos_diff = positions.diff()

# Add `holdings` to portfolio
portfolio['holdings'] = (positions.multiply(trade_data['Settle'], axis=0)).sum(axis=1)

# Add `cash` to portfolio
portfolio['cash'] = initial_capital - (pos_diff.multiply(trade_data['Settle'], axis=0)).sum(axis=1).cumsum()   

# Add `total` to portfolio
portfolio['total'] = portfolio['cash'] + portfolio['holdings']

# Add `returns` to portfolio
portfolio['returns'] = portfolio['total'].pct_change()

# Print the first lines of `portfolio`
portfolio

# Create a figure
fig = plt.figure(figsize=(20,6))

ax1 = fig.add_subplot(111, ylabel='Portfolio value in $')

# Plot the equity curve in dollars
portfolio['total'].plot(ax=ax1, lw=2.)

ax1.plot(portfolio.loc[signals.positions == 1.0].index, 
         portfolio.total[signals.positions == 1.0],
         '^', markersize=10, color='g')
ax1.plot(portfolio.loc[signals.positions == -1.0].index, 
         portfolio.total[signals.positions == -1.0],
         'v', markersize=10, color='k')

# Show the plot
plt.show()

# Isolate the returns of your strategy
returns = portfolio['returns']

# annualized Sharpe ratio
sharpe_ratio = np.sqrt(252) * (returns.mean() / returns.std())

# Print the Sharpe ratio
print(round(sharpe_ratio, 2))

# Create a figure
fig = plt.figure(figsize=(20,6))

# Define a trailing 252 trading day window
window = 252

# Calculate the max drawdown in the past window days for each day 
rolling_max = NG['Settle'].rolling(window, min_periods=1).max()
daily_drawdown = NG['Settle']/rolling_max - 1.0

# Calculate the minimum (negative) daily drawdown
max_daily_drawdown = daily_drawdown.rolling(window, min_periods=1).min()

# Plot the results
daily_drawdown.plot()
max_daily_drawdown.plot()

# Show the plot
plt.show()

# Get the number of days in `aapl`
days = (NG.index[-1] - NG.index[0]).days

# Calculate the CAGR 
cagr = ((((NG['Settle'][-1]) / NG['Settle'][1])) ** (365.0/days)) - 1

# Print the CAGR
print(round(cagr, 2))